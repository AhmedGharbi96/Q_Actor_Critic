{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self,state_size,action_size,path=None):\n",
    "            self.graph=tf.Graph()\n",
    "            self.sess=tf.Session()#(graph=self.graph)\n",
    "            self.path=path\n",
    "            self.state_size=state_size\n",
    "            self.action_size=action_size\n",
    "            self.gamma=0.95\n",
    "            #self.sess.run(tf.global_variables_initializer())\n",
    "            self.critic_model=self.build_model()\n",
    "            self.output_dim=self.critic_model.get_output_shape_at(-1)[1]\n",
    "            self.input_dim=self.critic_model.get_input_at(0).shape\n",
    "            self.lr=0.001\n",
    "            if (self.path!=None):\n",
    "                self.critic_model.load_weights(self.path)\n",
    "            self.td_err,self.grads,self.Q_action,self.init=self.update_critic_graph()\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def build_model(self):\n",
    "        #with self.graph.as_default():\n",
    "            model_input=Input(shape=(self.state_size,))\n",
    "            y1=Dense(50,activation=\"relu\")(model_input)\n",
    "            y2=Dense(self.action_size,activation=\"linear\")(y1)\n",
    "            model=Model(inputs=[model_input],outputs=[y2])\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            return(model)\n",
    "    def update_critic_graph(self):#,reward,state,action,next_state,next_action):\n",
    "        #with self.graph.as_default():\n",
    "            #tf.reset_default_graph()\n",
    "            Q_input_placeholder=tf.placeholder(shape=self.input_dim,dtype=tf.float32,name=\"Q_input_placeholder\")\n",
    "            Q_next_input_placeholder=tf.placeholder(shape=self.input_dim,dtype=tf.float32,name=\"Q_next_input_placeholder\")\n",
    "            Q=self.critic_model(Q_input_placeholder)\n",
    "            Q_next=self.critic_model(Q_next_input_placeholder)\n",
    "            action_one_hot_placeholder=tf.placeholder(shape=(None,self.output_dim),dtype=tf.float32,name=\"action_one_hot_placeholder\")\n",
    "            next_action_one_hot_placeholder=tf.placeholder(shape=(None,self.output_dim),dtype=tf.float32,name=\"next_action_one_hot_placeholder\")\n",
    "            Q_action=Q*action_one_hot_placeholder\n",
    "            Q_action=tf.keras.backend.sum(Q_action,axis=1)\n",
    "            Q_next_action=Q_next*next_action_one_hot_placeholder\n",
    "            Q_next_action=tf.keras.backend.sum(Q_next_action,axis=1)\n",
    "            td_err=self.gamma*Q_next_action-Q_action\n",
    "            grads=tf.gradients(Q_action,self.critic_model.trainable_weights)\n",
    "            init=tf.global_variables_initializer()\n",
    "            return(td_err,grads,Q_action,init)\n",
    "    \n",
    "    def update_critic(self,reward,state,action,next_state,next_action):\n",
    "        #self.sess.run(tf.global_variables_initializer())\n",
    "        #with self.graph.as_default():\n",
    "        td_zero_error,my_grads,value_action=self.sess.run([self.td_err,self.grads,self.Q_action],feed_dict={\n",
    "        \"Q_input_placeholder:0\":state.reshape((1,state.shape[0])),\"Q_next_input_placeholder:0\":next_state.reshape((1,next_state.shape[0])),\"action_one_hot_placeholder:0\":\n",
    "        utils.to_categorical(action, num_classes=self.output_dim).reshape(1,self.output_dim),\n",
    "        \"next_action_one_hot_placeholder:0\":utils.to_categorical(next_action, num_classes=self.output_dim).reshape(1,self.output_dim)}) \n",
    "        td_zero_error+=reward\n",
    "        new_weights=[self.lr*td_zero_error[0]*x+y for x,y in zip(my_grads,self.critic_model.get_weights())]\n",
    "        self.critic_model.set_weights(new_weights)\n",
    "        return(value_action[0])\n",
    "    \n",
    "    \n",
    "        \n",
    " \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,state_size,action_size,path=None):\n",
    "            self.path=path\n",
    "            self.state_size=state_size\n",
    "            self.action_size=action_size\n",
    "            self.sess=tf.Session()\n",
    "            self.actor_model=self.build_model()\n",
    "            self.output_dim=self.actor_model.get_output_shape_at(-1)[1]\n",
    "            self.input_dim=self.actor_model.get_input_at(0).shape\n",
    "            self.lr=0.001\n",
    "            if (self.path!=None):\n",
    "                self.critic_model.load_weights(self.path)\n",
    "            self.grads,self.init=self.update_actor_graph()\n",
    "            self.sess.run(self.init)\n",
    "\n",
    "    def predict(self,state):\n",
    "        return(self.actor_model.predict(state.reshape((1,state.shape[0]))))\n",
    "    \n",
    "    def build_model(self):\n",
    "        model_input=Input(shape=(self.state_size,))\n",
    "        y1=Dense(50,activation=\"relu\")(model_input)\n",
    "        y2=Dense(self.action_size,activation=\"softmax\")(y1)\n",
    "        model=Model(inputs=[model_input],outputs=[y2])\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        return(model)\n",
    "    def update_actor_graph(self):#,state,action,action_value):\n",
    "        state_placeholder=tf.placeholder(shape=self.input_dim,dtype=tf.float32,name=\"state_placeholder1\")\n",
    "        actions_prob=self.actor_model(state_placeholder)\n",
    "        action_one_hot_placeholder=tf.placeholder(shape=(None,self.output_dim),dtype=tf.float32,name=\"action_one_hot_placeholder1\")\n",
    "        action_prob=actions_prob*action_one_hot_placeholder\n",
    "        action_prob=tf.keras.backend.sum(action_prob,axis=1)\n",
    "        action_prob_log=tf.keras.backend.log(action_prob)\n",
    "        grads=tf.gradients(action_prob_log,self.actor_model.trainable_weights)\n",
    "        #self.sess.run(tf.global_variables_initializer())\n",
    "        init=tf.global_variables_initializer()\n",
    "        return(grads,init)\n",
    "       \n",
    "    def update_actor(self,state,action,action_value):\n",
    "        my_grads=self.sess.run([self.grads],feed_dict={\n",
    "        \"state_placeholder1:0\":state.reshape((1,state.shape[0])),\"action_one_hot_placeholder1:0\":\n",
    "        utils.to_categorical(action, num_classes=self.output_dim).reshape(1,self.output_dim)}) \n",
    "        new_weights=[self.lr*action_value*x+y for x,y in zip(my_grads[0],self.actor_model.get_weights())]\n",
    "        self.actor_model.set_weights(new_weights)\n",
    "                \n",
    "     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic:\n",
    "    def __init__(self,Actor,Critic):\n",
    "        self.actor=Actor\n",
    "        self.critic=Critic\n",
    "        self.env=gym.make(\"MountainCar-v0\")\n",
    "        self.actor_path=\"D:/RL_CartPole_agent_weights/MountainCar_actor.h5\"\n",
    "        self.critic_path=\"D:/RL_CartPole_agent_weights/MountainCar_critic.h5\"\n",
    "    def generate_episode(self,steps=200):\n",
    "        print(\"generate_episode_begin\")\n",
    "        state=self.env.reset()\n",
    "        self.max_pos=-0.4\n",
    "        actions=[]\n",
    "        visited_states=[]\n",
    "        rewards=[]\n",
    "        for step in range(steps):\n",
    "            prediction=self.actor.predict(state)\n",
    "            action=np.random.choice([0,1,2],p=prediction[0].reshape((self.actor.output_dim,)))\n",
    "            next_state,reward,done,_=self.env.step(action)\n",
    "            # Adjust reward based on car position\n",
    "            if (next_state[0]>self.max_pos):\n",
    "                self.max_pos=next_state[0]\n",
    "                #print(\"new max_pos={}\".format(self.max_pos))\n",
    "                reward+=1\n",
    "    \n",
    " \n",
    "        \n",
    "            if (next_state[0]>=0.5):\n",
    "                print(\"goal achieved\")\n",
    "                reward+=10\n",
    "                self.max_pos=-0.4\n",
    "                break\n",
    "\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            visited_states.append(state)\n",
    "            state=next_state\n",
    "        print(\"new max_pos={}\".format(self.max_pos))\n",
    "        return (visited_states,actions,rewards)\n",
    "    \n",
    "    def train_AC(self,n_episodes=1000):\n",
    "        for episode in range(n_episodes):\n",
    "            states,actions,rewards=self.generate_episode(steps=200)\n",
    "            \n",
    "            for i in range(len(states)-1):\n",
    "                action_value=self.critic.update_critic(rewards[i],states[i],actions[i],states[i+1],actions[i+1])\n",
    "                self.actor.update_actor(states[i],actions[i],action_value)\n",
    "                if(i%50==0):\n",
    "                    print(\"step{}/{}\".format(i,len(states)))\n",
    "            print(\"episode {}/{}\".format(episode+1,n_episodes))\n",
    "            if episode % 5 == 0:\n",
    "                self.actor.actor_model.save_weights(self.actor_path)\n",
    "                self.critic.critic_model.save_weights(self.critic_path)\n",
    "            \n",
    "        return(states,actions,rewards)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_episode_begin\n",
      "new max_pos=-0.3662800421292447\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 1/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 2/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.35712086407262833\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 3/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3896227456874627\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 4/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3683875227901036\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 5/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3719411255359379\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 6/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.35612548699052243\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 7/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 8/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 9/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 10/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 11/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3261653707459902\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 12/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 13/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 14/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.38487088691618315\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 15/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.39196220807252397\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 16/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.39913715459134247\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 17/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 18/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 19/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3983237016657326\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 20/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.2894619221633956\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 21/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.31471335038119275\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 22/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 23/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.381006830414171\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 24/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3262402480985607\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 25/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 26/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 27/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 28/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3525812641138849\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 29/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 30/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 31/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 32/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3561373375200918\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 33/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 34/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 35/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.3323250463734814\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 36/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.36807906119663003\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 37/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 38/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n",
      "step150/200\n",
      "episode 39/1000\n",
      "generate_episode_begin\n",
      "new max_pos=-0.4\n",
      "step0/200\n",
      "step50/200\n",
      "step100/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5fae52132289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mAC_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mActor_Critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAC_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_AC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-ee0b65fe7409>\u001b[0m in \u001b[0;36mtrain_AC\u001b[1;34m(self, n_episodes)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0maction_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"step{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-64494b10066d>\u001b[0m in \u001b[0;36mupdate_actor\u001b[1;34m(self, state, action, action_value)\u001b[0m\n\u001b[0;32m     39\u001b[0m         my_grads=self.sess.run([self.grads],feed_dict={\n\u001b[0;32m     40\u001b[0m         \u001b[1;34m\"state_placeholder1:0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"action_one_hot_placeholder1:0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         utils.to_categorical(action, num_classes=self.output_dim).reshape(1,self.output_dim)}) \n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mnew_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env=gym.make(\"MountainCar-v0\")\n",
    "state_size=env.observation_space.shape[0]\n",
    "action_size=env.action_space.n\n",
    "actor=Actor(state_size,action_size)\n",
    "critic=Critic(state_size,action_size)\n",
    "AC_agent=Actor_Critic(actor,critic)\n",
    "states,actions,rewards=AC_agent.train_AC()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
